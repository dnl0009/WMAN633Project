---
title: "Quantitative Ecology (WMAN 633) Course Project Code"
author: "Darien Lozon"
date: "April 10, 2019"
output: html_document
---

This first section of code was necessary to isolate the song sparrow abundance data from the mass data set. 
```{r}
####### SET THE STAGE ########
setwd("C:/Users/dnl0009/Google Drive/SP19 Courses/WMAN633/Project/")
season1 <- read.csv("Data 2016-2017.csv",header=T)
season2 <- read.csv("Data 2017-2018.csv",header=T)
# Clean up data; I don't need notes or easement info
# I also don't need the deviation from 1st and 2nd rounds.

# Katy used the flock column different from how I did.. 
# I'll get rid of the flock column and create a number column (which is essentially how I am used to using the flock column).. it'll help when I aggregate.

season1 <- season1[c(1:1028,1031:2371),c(1:9,11:12)] 
# Something was funky when calling this csv in, so I had to be picky about which columns and rows to bring in

####### CLEAN UP DATA #######
season1$Temp.C <- round(season1$Temp.C,3)

# Have to make the observer names consistent (Observer could potentially be a detection covariate)
unique(season1$Observer) # The observer codes are a mess
season1$Observer[season1$Observer == "kel"] = 'KEL'
season1$Observer[season1$Observer == "KEL, CDH"] = 'KEL,CDH' # Can't create new levels
season1$Observer[season1$Observer == "KEL/CDH"] = 'KEL,CDH'  # Well, you can, but I don't know how
season1$Observer[season1$Observer == "KEL, TC"] = 'KEL,TC'   # So we'll work with what's already there
season1$Observer <- factor(season1$Observer) # All fixed

# Names in the excel spreadsheet were not consistent either
unique(season1$Site) 
season1$Site[season1$Site == 'dnr ref '] = 'dnr ref'
season1$Site[season1$Site == 'Shabb reference'] = 'shabb ref'
season1$Site[season1$Site == 'Moore-ref'] = 'Moore ref'
season1$Site[season1$Site == 'Wolfe-ref'] = 'Wolfe ref'
season1$Site[season1$Site == 'sheets-brock'] = 'Sheets Brock'
season1$Site[season1$Site == 'Burr-ref'] = 'Burr ref'
season1$Site[season1$Site == 'Thorn Reference'] = 'Thorn ref'
season1$Site[season1$Site == 'allen'] = 'Allen'
season1$Site[season1$Site == 'sites'] = 'Sites'
season1$Site <- factor(season1$Site) # Good deal

unique(season1$Spp) # I'm afraid to look

# Okay, here we go with the crazy embedded ifelse for a number column we'll aggregate later..
# If someone has a more efficient way to do this, I'm all ears, but this ifelse statement is oddly beautiful.
season1$num <- ifelse(season1$Spp == 'AMGO x9','9',
                      ifelse(season1$Spp == 'CANG x14','14',
                             ifelse(season1$Spp == 'AMGO x7','7',
                                    ifelse(season1$Spp == 'CANG x21','21',
                                           ifelse(season1$Spp == 'MALL x11','11',
                                                  ifelse(season1$Spp == 'CANG x20','20',
                                                         ifelse(season1$Spp == 'UNCH x6','6',
                                                                ifelse(season1$Spp == 'AMCR x4','4',
                                                                       ifelse(season1$Spp == 'AMCR x3','3',
                                                                              ifelse(season1$Spp == 'AMRO x5','5',
                                                                                     ifelse(season1$Spp == 'BLJA x5','5',
                                                                                            ifelse(season1$Spp == 'BLJA x2','2',
                                                                                                   ifelse(season1$Spp == 'DEJU x6','6',
                                                                                                          ifelse(season1$Spp == 'AMGO x2','2',
                                                                                                                 ifelse(season1$Spp == 'BLJA x4','4',
                                                                                                                        ifelse(season1$Spp == '', '0','1')))))))))))))))) # They recorded when no birds were observed with a blank space

## Now that the numbers are accounted for, I can change the names of the species
season1$Spp[season1$Spp == 'AMGO x9'] = 'AMGO'
season1$Spp[season1$Spp == 'CANG x14'] = 'CANG' # CAGO could be cackling or canada, yikes.
season1$Spp[season1$Spp == 'AMGO x7'] = 'AMGO'
season1$Spp[season1$Spp == 'CANG x21'] = 'CANG'
season1$Spp[season1$Spp == 'MALL x11'] = 'MALL'
season1$Spp[season1$Spp == 'CANG x20'] = 'CANG'
season1$Spp[season1$Spp == 'SOSP '] = 'SOSP'

# I don't think I want any of the unknown anythings, but I'll keep them for now.
season1$Spp[season1$Spp == 'UNCH x6'] = 'UNCH'
levels(season1$Spp)[levels(season1$Spp) == 'unwo'] = 'UNWO'

season1$Spp[season1$Spp == 'AMCR x4'] = 'AMCR'
season1$Spp[season1$Spp == 'AMCR x3'] = 'AMCR'
season1$Spp[season1$Spp == 'AMRO x5'] = 'AMRO'
season1$Spp[season1$Spp == 'BLJA x5'] = 'BLJA'
season1$Spp[season1$Spp == 'BLJA x2'] = 'BLJA'
season1$Spp[season1$Spp == 'BLJA x4'] = 'BLJA'
season1$Spp[season1$Spp == 'DEJU x6'] = 'DEJU'
season1$Spp[season1$Spp == 'AMGO x2'] = 'AMGO'
season1$Spp[season1$Spp == 'AMGO '] = 'AMGO'
season1$Spp[season1$Spp == 'AMCR '] = 'AMCR'

# Need to account for absences as well
levels(season1$Spp)[levels(season1$Spp) == ''] = 'SOSP' 
season1$Spp <- factor(season1$Spp)

season1$Date <- as.Date(season1$Date,format='%m/%d/%Y') # R friendly reformatting
season1$Date <- as.factor(season1$Date) # But I do want dates as factors 

# Fixing date
season1$Date[season1$Date == '0016-12-20'] = '2016-12-20'
season1$Date[season1$Date == '0017-01-13'] = '2017-01-13'
season1$Date[season1$Date == '0017-01-14'] = '2017-01-14'
season1$Date <- factor(season1$Date)

# This is for changing levels themselves... different from the commands above (learn something new every day)
levels(season1$Date)[levels(season1$Date) == "0017-01-15"] = "2017-01-15"
levels(season1$Date)[levels(season1$Date) == "0017-01-20"] = "2017-01-20"
levels(season1$Date)[levels(season1$Date) == "0017-01-21"] = "2017-01-21"
levels(season1$Date)[levels(season1$Date) == "0017-01-28"] = "2017-01-28"
levels(season1$Date)[levels(season1$Date) == "0017-01-29"] = "2017-01-29"
levels(season1$Date)[levels(season1$Date) == "0017-02-03"] = "2017-02-03"
levels(season1$Date)[levels(season1$Date) == "0017-02-04"] = "2017-02-04"

season1$Date <- as.Date(season1$Date,format='%Y-%m-%d') # Don't think this needs to be as.date, but why not?
```

---

Now that the data is coherent and the levels of the categories are consistent throughout the entire dataset, I can begin to aggregate by point count and date combinations.

```{r}
####### POINT COUNT AGGREGATION #######
# Try to aggregate the whole season1 dataset and calculate abundance by POINT COUNT because each point count is going to be treated as an individual event.

season1$num <- as.numeric(season1$num) # This was to ensure values were interpreted as integers

## Aggregation ##
bypt <- aggregate(season1$num,by=list(season1$Date,season1$Point,season1$Spp),FUN=sum)
bypt <- bypt[with(bypt,order(Group.1,Group.2)),] # Something I found on stackoverflow to order data chronologically, which doesn't really matter in the end
colnames(bypt) <- c("Date","Point","Spp","Num")
head(bypt) 
sosp <- bypt[bypt$Spp == 'SOSP',] # But this doesn't account for sites that didn't have SOSP (absence is important)

```
This is what I want, but I also want the other data associated with each date/point combination; that's where the p and psi covariate datasets come into play later.

---

I thought mean maximum height of vegetation at each point location could be a worthwhile site covariate to consider, but when I added it to the unmarkedFramePCount later, it came back as an error, so I omitted it. 
````{r}
# Pull in data and reformat #
veg1 <- read.csv("Vegetation Data 2016-2017.csv",header=TRUE)
levels(veg1$date) #Check date formats %y vs %Y
veg1$date <- as.Date(veg1$date,format="%m/%d/%Y")
veg1$date <- as.factor(veg1$date)
levels(veg1$date)[levels(veg1$date) == '0017-01-14'] = '2017-01-14' # The only date that needs to be changed
veg1$date <- as.Date(veg1$date,format="%Y-%m-%d")
veg1 <- veg1[,1:7] # Weird x column with a bunch of NA values

# For some reason, when directly changing the max ht to numeric, it converted the levels, not the values... 
veg1$max.height <- as.character(veg1$max.height) # ... so I had to change the factor to character to eliminate that problem.
veg1$max.height <- as.numeric(veg1$max.height)# Here we go. Now we can aggregate by point site, just like the bird counts.

# Aggregate #
veg1 <- aggregate(veg1$max.height,by=list(veg1$date,veg1$sampling.pt),FUN=mean)
veg1 <- veg1[with(veg1,order(Group.1)),]
colnames(veg1) <- c("Date","Point","Mean.Max.Ht")
head(veg1)
veg1$Mean.Max.Ht <- round(veg1$Mean.Max.Ht,3) # Round nasty decimals
head(veg1) 
```

---

After a while, I began making little celebratory checkpoints. Here is one.
```{r}
###### CATCHING UP ######
setwd("C:/Users/dnl0009/Google Drive/SP19 Courses/WMAN633/Project/")
load('bird.RData') # sosp,tab,bypt,season1,veg1

```

---

The problem I needed to address with the bird data was the transect counts. To increase detection probability, field researchers treat the distance between each point count as a transect, but psi covariates such as vegetative density (likely measured using a Robel pole) were not measured at every possible location within each transect. With the following code, I deleted those transect birds.
```{r}
sosp$rowname <- rownames(sosp) # Need a form of ID for the transect point names

# Isolate rows with transect data (this will need to be done again later for a different reason)
tr <- c(6 ,7, 8,18,19,20,21,22,26,27,28,29,30,31,35,36,46,47,48,49,50,
        51,52,53,54,55,56,67,68,69,74,75,76,77,78,80,82,85,86,87,88,90,
        105,106,107,108,109,110,111,112,113,114,118,119,120,127,128,129,130,137,138)

# Take them out
upd_sosp <- sosp[!(sosp$rowname %in% tr),]

# Check work
unique(upd_sosp$Point) # Voici! 
upd_sosp <- upd_sosp[,c(1,2,4)] # Can get rid of the rowname and spp columns
upd_sosp$Point <- factor(upd_sosp$Point) # Update point levels
head(upd_sosp)
```

---

These merges were used to identify ALL point count locations that WERE points (rather than transects) that either did (>1 bird) or did NOT (0 birds) have song sparrows. Sometimes the point count would be skipped if there were no observations. Merging it with the whole list of possible point counts will give me the zeros I need.
```{r}
pts <- matrix(unique(season1$Point),ncol=1) # Point location names
colnames(pts) <- c('Point')

# Separate the two visits for each point, so I can create two columns in a data frame.
round1 <- upd_sosp[1:84,]; round2 <- sosp[85:138,] 
test <- merge(pts,round1,by='Point',all=TRUE); head(test) 
test <- merge(test,round2,by='Point',all=TRUE); head(test)
test <- test[,c(1,3,6)]; head(test) # We only need the count values and ONE column of point names
test[is.na(test)] <- 0 # Replace NAs with 0 values because SOSP were not counted at that point
# The replacement of NA <- 0 comes back with an error, but we can see that it worked.

colnames(test) <- c('point','sosp1','sosp2')
head(test) # AWESOME

# Here's that eliminating of transect data again...
test$rowname <- rownames(test) # Need a form of ID for the transect point names

# Isolate rows with transect data to eliminate later
tr <- c(7:12,25:32,39:44,50:53,58:60,72:80,92:98,109:116,124:126,
        130:131,142:150,155:156,160:161,
        178:187,197:203,212:217,221,228:239)
transects <- test$point[tr]

# Take them out
SOSP <- test[!(test$rowname %in% tr),]; head(SOSP,20) # HUZZAAHHH!
# Checkpoint: save(bypt,season1,SOSP,sosp,test,upd_sosp,veg1,tr,transect,pts,tab,file='bird.RData')
```

---

Another issue that arose with the data was not every point count location had p or psi covariates. Obviously, these point locations could not be included in the model. 
```{r}
# What point counts are missing from the p or psi covariates?
psi_covs <- read.csv('psi_covs.csv',header=TRUE); psi_covs <- psi_covs[psi_covs$year =='0',]
p_covs <- read.csv('p_covs.csv',header=TRUE); p_covs <- p_covs[1:87,]
trees <- read.csv('trees.csv',header=TRUE); colnames(trees) <- c('site','date','point','count')

check <- merge(SOSP,p_covs,by='point',all.x=TRUE)
check <- merge(check,trees,by='point',all.x=TRUE)
check <- merge(check,psi_covs,by='point',all.x=TRUE); View(check)
check <- na.omit(check); View(check)
```

Once I checked the lengths (because I didn't do this before, I got angry messages in unmarked that there were inconsistent lengths), I could break up the massive data frame I just made above into the separate sections I need for the unmarkedFramePCount.
```{r}
# Separate data frames
SOSP <- check[,2:3]
p_covs <- check[,c(5:25)]
psi_covs <- check[,26:38]
# YAAAAAAAS.
```

---

Here comes unmarked.

```{r}
library(unmarked)
SOSP <- as.matrix(SOSP) # make matrix
sosp_dat <- unmarkedFramePCount(y=SOSP)
fit <- pcount(~1~1,sosp_dat,K=100) # Intercept only model

# Model and interpretation
summary(fit)
plogis(-0.0534) # Detection probability (49%)
exp(0.674) # Expected abundance (2 birds)

# Check classes of psi_covs
psi_covs$reclass_bg1 <- factor(psi_covs$reclass_bg1)
psi_covs$reclass_shr1 <- factor(psi_covs$reclass_shr1)
psi_covs$reclass_heb1 <- factor(psi_covs$reclass_heb1)
psi_covs$reclass_wood1 <- factor(psi_covs$reclass_wood1)
psi_covs$reclass_wat1 <- factor(psi_covs$reclass_wat1)
psi_covs$reclass_shr5 <- factor(psi_covs$reclass_shr5)
psi_covs$type <- factor(psi_covs$type)

# Check classes of p_covs
p_covs$sky.1 <- factor(p_covs$sky.1)
p_covs$sky.2 <- factor(p_covs$sky.2)

det_covs <- list( # det_covs chosen from my listed hypotheses
  time = data.frame(p_covs[,c('time.1','time.2')]),
  sky = data.frame(sky.1 = factor(p_covs$sky.1),
                   sky.2 = factor(p_covs$sky.2)),
  wind = data.frame(p_covs[,c('wind.1','wind.2')]),
  temp = data.frame(p_covs[,c('temp.1','temp.2')]),
  dist = data.frame(p_covs[,c('dist.1','dist.2')])
)

nmix_dat <- unmarkedFramePCount(y=SOSP,            # abundance
                                siteCovs=psi_covs, # site covs
                                obsCovs=det_covs)  # detection covs
FIT_FINALLY <- pcount(~temp+wind+sky~count+reclass_shr5+reclass_wat1+reclass_heb1,
                      nmix_dat,K=100)
summary(FIT_FINALLY) # This is the global fit Poisson model, just to ensure for my sanity that it works, and it DID! :)
```
 
Because of the excessive zeros in my data, I believe it would be beneficial to compare not only Poisson models, but also Negative-Binomial (as shown in lecture) and Zero-Inflated Poisson. Fit 1-6 are consistent and were created via backward elimination of both detection and abundance covariates.
```{r}
# Poisson fit models
fit1 <- pcount(~time+temp+sky+dist+wind~size+type+reclass_wat1+reclass_shr1+reclass_shr5,nmix_dat,K=100)
fit2 <- pcount(~time+temp+dist+wind~type+reclass_wat1+reclass_shr1+reclass_shr5,nmix_dat,K=100)
fit3 <- pcount(~time+dist+wind~type+reclass_shr1+reclass_shr5,nmix_dat,K=100)
fit4 <- pcount(~time+dist+wind~type+reclass_shr5,nmix_dat,K=100)
fit5 <- pcount(~time+dist+wind~type+reclass_shr1,nmix_dat,K=100)
fit6 <- pcount(~time+dist~type+reclass_shr1+reclass_shr5,nmix_dat,K=100)

# Negative binomial models
fit1N <- pcount(~time+temp+sky+dist+wind~size+type+reclass_wat1+reclass_shr1+reclass_shr5,nmix_dat,K=100,"NB")
fit2N <- pcount(~time+temp+dist+wind~type+reclass_wat1+reclass_shr1+reclass_shr5,nmix_dat,K=100,"NB")
fit3N <- pcount(~time+dist+wind~type+reclass_shr1+reclass_shr5,nmix_dat,K=100,"NB")
fit4N <- pcount(~time+dist+wind~type+reclass_shr5,nmix_dat,K=100,"NB")
fit5N <- pcount(~time+dist+wind~type+reclass_shr1,nmix_dat,K=100,"NB")
fit6N <- pcount(~time+dist~type+reclass_shr1+reclass_shr5,nmix_dat,K=100,"NB")

# Zero-inflated Poisson models
fit1Z <- pcount(~time+temp+sky+dist+wind~size+type+reclass_wat1+reclass_shr1+reclass_shr5,nmix_dat,K=100,"ZIP")
fit2Z <- pcount(~time+temp+dist+wind~type+reclass_wat1+reclass_shr1+reclass_shr5,nmix_dat,K=100,"ZIP")
fit3Z <- pcount(~time+dist+wind~type+reclass_shr1+reclass_shr5,nmix_dat,K=100,"ZIP")
fit4Z <- pcount(~time+dist+wind~type+reclass_shr5,nmix_dat,K=100,"ZIP")
fit5Z <- pcount(~time+dist+wind~type+reclass_shr1,nmix_dat,K=100,"ZIP")
fit6Z <- pcount(~time+dist~type+reclass_shr1+reclass_shr5,nmix_dat,K=100, "ZIP")
# Checkpoint save.image(file='LozonProjectCode.RData')
```

Once these models were fit, I compared AIC values to identify which model fits best.
```{r}
library(AICcmodavg)
cand.set <- list(
  M1 = fit1, M2 = fit2, M3 = fit3, M4 = fit4, M5 = fit5, M6 = fit6, # Regular poisson
  M1N = fit1N, M2N = fit2N, M3N = fit3N, M4N = fit4N, M5N = fit5N, M6N = fit6N, # Negative binom
  M1Z = fit1Z, M2Z = fit2Z, M3Z = fit3Z, M4Z = fit4Z, M5Z = fit5Z, M6Z = fit6Z # Zero-inflated poisson
)
mods <- aictab(cand.set=cand.set,second.ord=F); mods
```

The model results are intriguing... The global models M1x are obviously penalized likely because of overparameterization, and no surprise that the top five models are either ZIP or NB models, but the significance of variables (yes, crossing snares of paradigms) are confusing... I began calculating model averages for primary covariates:
```{r}
# Detection
time <- modavgShrink(cand.set=cand.set,
                     parm = 'time',
                     second.ord=F,
                     parm.type='detect')
dist <- modavgShrink(cand.set=cand.set,
                     parm = 'dist',
                     second.ord=F,
                     parm.type='detect')
wind <- modavgShrink(cand.set=cand.set,     # Wind was surprising to me. Even the average
                     parm = 'wind',         # was positive. I thought it would only be 
                     second.ord=F,          # positive given the other covariates fit... hmm..
                     parm.type='detect')

# Comparison is not too far off.
detcoef <- list(time$Mod.avg.beta,dist$Mod.avg.beta,wind$Mod.avg.beta); detcoef
coef(fit3Z)[7:9] 

# Lambda

# We did 'type' in class, so I will not analyze it here.

shr1 <- modavgShrink(cand.set=cand.set,
                     parm = 'reclass_shr12',
                     second.ord=F,
                     parm.type='lambda')
shr52 <- modavgShrink(cand.set=cand.set,
                     parm = 'reclass_shr52',
                     second.ord=F,
                     parm.type='lambda')
shr53 <- modavgShrink(cand.set=cand.set,
                     parm = 'reclass_shr53',
                     second.ord=F,
                     parm.type='lambda')
```
```{r,echo=F}
shr1;shr52;shr53
```

---

I decided to do the same calculations for covariates that were only in the global model and __one__ other model.
```{r}
# Second-order covariate removals from both detection and abundance?
temp <- modavgShrink(cand.set=cand.set,
                     parm = 'temp',
                     second.ord=F,
                     parm.type='detect')

water <- modavgShrink(cand.set=cand.set,
                      parm = 'reclass_wat11',
                      second.ord=F,
                      parm.type='lambda')
```
```{r,echo=F}
temp;water
```


Why were these removed from the global model?

Temperature and water __both__ end up being positive (small, but positive), which means they PROMOTE detection probability and abundance, respectively, but they were not incorporated in other models because of high p-values in the models they were in (M1x and M2x). There is another reason!! In the model averaging output for both temp and water, their 95% confidence intervals include 0, which means they are insignificant.

The fuzziness is apparent when trying to make an inference and can be a reason why it is not wise to incorporate the two paradigms of p-values and model selection because there can be parameters that are positive but do not reach the "significance" threshold.

---

Le fin.